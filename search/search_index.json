{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab Documentation","text":"<p>Welcome to the docs for my homelab!</p> <p>These docs will serve as both a reference for me, and a possible place for people to start their own homelabs.</p>"},{"location":"#contact","title":"Contact","text":"<p>If you would like to get in contact with me, you can contact me via:</p> <p>Discord</p> <p>Email</p>"},{"location":"#projects","title":"Projects","text":""},{"location":"#redbrick","title":"Redbrick","text":"<p>Redbrick is DCU's Computing and Networking Society. I am a SysAdmin there where I run 13 servers with a range of services from Email to markdown editors. I work with Ubuntu, Debian and NixOS and it is all voluntary. Some of our best achievements include resolving an issue with Mailman - our mailing list provider. You can read about our journey here.</p>"},{"location":"procedures/","title":"Procedures","text":"<p>This section will document general procedures on how my homelab functions.</p>"},{"location":"procedures/#backups","title":"Backups","text":"<p>In general, anything that needs a backup is stored in <code>/volume1/backups/[service]</code> on Dionysus. This is a 1TB folder which is encrypted on the NAS.</p> <p>Most backup scripts follow a similar format.</p> <ol> <li>Find any file, or set of files that are older than the retention time, and delete them.</li> <li>Copy the new backup to the mounted folder, and find the file size of that backup.</li> <li>Send a notification to Discord containing the service that was backed up, the file size and the time and date.</li> </ol> <p>These scripts run with cron at the desired backup interval. Each service has a dedicated section on how it is backed up. Refer to those to find out more on a service-by-service breakdown.</p>"},{"location":"procedures/#step-by-step","title":"Step by step","text":"<ol> <li>Create a backups folder on host (has to be empty) and a corresponding folder on NAS</li> <li>Run <code>sudo apt install cifs-utils</code></li> <li>Run <code>mount -t cifs -o username=distro \"\\\\\\\\dionysus\\\\backups\\\\[service]\" '/location/of/backup'</code></li> <li>Edit <code>/etc/fstab</code> to contain connection settings <code>//dionysus/backups/[service] /location/of/backup cifs credentials=/etc/win-credentials, file_mode=0755,dir_mode=0755 0 0</code></li> <li>Create <code>/etc/win-credentials</code> and add <code>username=[username] password=password</code></li> <li>Create a script to notify Discord and remove old backups.</li> </ol>"},{"location":"procedures/#restoring-a-backup","title":"Restoring A Backup","text":"<p>This depends greatly on the service, as such each service section has a <code>Restoration</code> section. Refer to those sections to understand how this works.</p>"},{"location":"procedures/#connecting-to-my-homelab","title":"Connecting to my HomeLab","text":"<p>I have set up my network and DNS in such a way to allow me to SSH to my HomeLab from anywhere in the world, except for China. To do this, I used Cloudflare to allow me to dynamically update my DNS entries, as my home IP is not static. Then I set up my domains to proxy through Cloudflare to hide my IP. Now you may be wondering how I connect home if my IP is hidden from the world. The answer is disabling Cloudflare proxy on a subdomain. This allows my IP to exposed through a domain name dynamically.</p>"},{"location":"redbrick/","title":"Redbrick","text":"<p>A collection of useful commands and helpful advice for both myself and aspiring admins.</p>"},{"location":"redbrick/#update-dns","title":"Update DNS","text":"<p>Connect to Paphos cd /etc/bind/master Backup zone file <code>db.Redbrick.dcu.ie</code> Run <code>rdnc freeze redbrick.dcu.ie</code> (produces no output) Update serial number (line 4, format: YYYYMMDD(number of changes)) Add new domain under the $ORIGIN that you need (based on \"tld\") Run <code>rndc thaw [domain]</code> <code>service bind9 status</code> <code>named-checkzone (zone file/db.Redbr...)</code> check zone to make sure it follows syntax and is valid record <code>var/log/named/(default.log)</code> are logs of dns errors (pretty shit)</p>"},{"location":"todo/","title":"TODO","text":"<ul> <li>The Forest Dedicated Server</li> <li>ELK<ul> <li>elastisearch</li> <li>kafka</li> <li>Logstash</li> </ul> </li> <li>TICK<ul> <li>Telegraf</li> <li>Influx</li> <li>Kapacitor</li> <li>Chronograph</li> </ul> </li> </ul>"},{"location":"todo/#notes","title":"Notes","text":"<p>To allow PiHole have iFrames edit <code>/etc/lighttpd/lighttpd.conf</code> and comment the line that says <code>\"X-Frame-Options\" =&gt; \"DENY\"</code>. Run <code>sudo service lighttpd restart</code> to apply change.</p>"},{"location":"todo/#network-topology","title":"Network Topology","text":""},{"location":"todo/#hardware","title":"Hardware","text":"<ul> <li>.1 router</li> <li>.2 hades (switch)</li> <li>.3 zeus (optiplex)</li> <li>.4 hermes (optiplex)</li> <li>.5 dionysus (NAS)</li> <li>.6 apollo (raspi 4b)</li> <li>.7 artemis (raspi 4b)</li> <li>.10 poseidon (PC)</li> <li>.11 perseus (Dell Laptop)</li> <li>.12 icarus (Galaxy Note 9)</li> <li>(.xxx) castor (Unifi AP Lite 6)</li> </ul>"},{"location":"todo/#vms","title":"VMs","text":"<ul> <li>hera - 4core 24GB</li> <li>aphrodite - 1core 1GB</li> <li>persephone - 1core 1GB</li> </ul>"},{"location":"todo/#other-hardware-names","title":"Other hardware names","text":"<ul> <li>ares</li> <li>hephaestus</li> <li>hercules</li> <li>atlas</li> <li>charon</li> <li>cronus</li> <li>helios</li> <li>hypnos</li> <li>pan</li> <li>polux</li> <li>prometheus</li> </ul>"},{"location":"todo/#other-vm-names","title":"Other VM names","text":"<ul> <li>demeter</li> <li>athena</li> <li>hestia</li> </ul>"},{"location":"todo/#next-pc-build","title":"Next PC Build","text":"<p>Case: SSUPD Meshalicious CPU: Ryzen 5 3600 GPU: GeForce GTX 1660ti RAM: 32GB</p>"},{"location":"hardware/device-list/","title":"Device List","text":"<p>List of current hardware on network that includes network equipment, computer and mobile devices and configuration details.</p>"},{"location":"hardware/device-list/#computers-and-mobile-devices","title":"Computers and Mobile Devices","text":""},{"location":"hardware/device-list/#computers-and-servers","title":"Computers and Servers","text":"Device Name CPU RAM OS Hostname Gaming PC Ryzen 5 3600 32GB DDR4 3200MHz Win 11 Poseidon OptiPlex 7040 Intel i5-6500T 8GB DDR4 Debian 12 Zeus OptiPlex 7040 Intel i5-6500T 8GB DDR4 Debian 12 Hermes Synology NAS Intel J4125 20GB SoDIMM Synology DSM7 Dionysus Raspberry Pi 4 BCM2711 (ARM) 2GB Raspbian 10 Apollo Raspberry Pi 4 BCM2711 (ARM) 2GB Raspbian 10 Artemis"},{"location":"hardware/device-list/#laptops","title":"Laptops","text":"Device Name CPU RAM OS Hostname Dell Inspiron Ryzen 5 550U 8GB DDR4 Windows 11 perseus"},{"location":"hardware/device-list/#connected-home-devices--speakers","title":"Connected Home Devices &amp; Speakers","text":"Device Name Location Nickname Govee Lights Bedroom Bedroom Lights Google Home Kitchen Kitchen Google Chromecast Living room Living room"},{"location":"hardware/device-list/#network-devices","title":"Network Devices","text":""},{"location":"hardware/device-list/#lan-devices","title":"LAN Devices","text":"Device Name # Ports Type Hostname Ubiquiti 16 Switch PoE Lite 16/8x PoE Switch Hades Ubiquiti AP WiFi 6 N/A AP Castor"},{"location":"hardware/device-list/#router-details","title":"Router Details","text":"Device Name Firewall DHCP Virgin Media Router Yes No"},{"location":"hardware/device-list/#images","title":"Images","text":"<p>Starting from the top left, there are two raspberry pis, a Ubiquiti switch, two Dell OptiPlex 7040s, and a Synology DS920+ On the right is my PC.</p>"},{"location":"hardware/laptop/","title":"Laptop // Perseus","text":""},{"location":"hardware/laptop/#specifications","title":"Specifications","text":"CPU RAM Storage OS Ryzen 5 5500U 8GB 256GB SSD Windows 11"},{"location":"hardware/optiplex-7040/","title":"Dell OptiPlex 7040 // Zeus + Hermes","text":"<p>These small servers are perfect for an ITX sized homelab. They provide enough computing power for most tasks and combined have 16GB of RAM. These servers were the first of my actual hardware I bought.</p>"},{"location":"hardware/optiplex-7040/#specifications","title":"Specifications","text":"CPU RAM Storage OS Hostname Intel i5-6500T 8GB 250GB SSD Debian 12 zeus Intel i5-6500T 8GB 250GB SSD Debian 12 hermes"},{"location":"hardware/optiplex-7040/#services","title":"Services","text":"<p>Both of these servers are part of a nomad datacentre that alloctes jobs on any available node according to the scheduler. You can find out more about the service running in this cluster here.</p>"},{"location":"hardware/optiplex-7040/#notes","title":"Notes","text":""},{"location":"hardware/optiplex-7040/#converting-from-windows-to-new-os","title":"Converting from Windows to New OS","text":"<ol> <li>Create a bootable USB with desired operating system</li> <li>When booting up, press F2</li> <li>Allow \"boot from USB\" in BIOS</li> <li>Follow installer instructions</li> </ol>"},{"location":"hardware/optiplex-7040/#images","title":"Images","text":""},{"location":"hardware/pc/","title":"PC // Poseidon","text":"<p>My personal computer is built in a k39 PC case, and is only 3.9 litres in volume. This was definitely a fun challenge to build in, but as hardware gets bigger, more power hungy and hotter, this form factor is more than likely dying out.</p> <p>I have plenty of photos of the computer in various stages of assembly.</p>"},{"location":"hardware/pc/#specifications","title":"Specifications","text":"CPU GPU RAM Storage OS Hostname Ryzen 5 3600 RTX 4070 32GB 3200MHz 2.5TB Windows 11 poseidon"},{"location":"hardware/pc/#images","title":"Images","text":""},{"location":"hardware/pc/#general-setup","title":"General Setup","text":""},{"location":"hardware/pc/#keyboard","title":"Keyboard","text":""},{"location":"hardware/pc/#case","title":"Case","text":""},{"location":"hardware/raspberry-pi/","title":"Raspberry Pis","text":"<p>The standard device in almost any homelab, you can never have enough of them until you have too many. I have 4. They all have PoE+ hats that allow them to be connected to the internet and get power with one cable.</p> CPU RAM Storage OS Extras Broadcom BCM2711 (ARM) 2GB 16GB USB Drive Raspbian 11 PoE+ Hat"},{"location":"hardware/raspberry-pi/#services","title":"Services","text":"<p>The services running on the Raspberry Pis are listed here. This list is updated frequently.</p> <p>As my HomeLab has two identical Raspberry Pis, it can run many services in a hot-hot/hot-cold format.</p> Service Name Description Highly Available? Hostname Pihole Network-wide adblocker and DHCP Server Yes Apollo/Artemis <p>Info</p> <p>Anything marked as Highly Available is run in either a hot-hot or hot-cold setup depending on the importance of the service.</p>"},{"location":"hardware/raspberry-pi/#images","title":"Images","text":""},{"location":"hardware/switch/","title":"Ubiquiti 16 Port Switch PoE Lite // Hades","text":"<p>A 16 Port PoE Switch that powers my Raspberry Pis, and breaks out ethernet to all my devices.</p>"},{"location":"hardware/switch/#images","title":"Images","text":""},{"location":"hardware/synology-nas/","title":"Synology DS920+ // Dionysus","text":"<p>This NAS was one of the best investments in my homelab that I made. It has allowed me learn about backups, centralised storage, and is the base for my media services.</p> CPU RAM Storage OS Intel J4125 20GB SoDIMM 16TB Raw/10TB RAID 5 Synology DSM 7"},{"location":"hardware/synology-nas/#services","title":"Services","text":"<p>The services running on the Synology DS920+ are listed here. This list is updated frequently.</p> Service Name Description ruTorrent download client Jackett torrent indexer Plex Plex homepage Sonarr TV show manager Radarr movie manager"},{"location":"hardware/synology-nas/#images","title":"Images","text":""},{"location":"metrics/","title":"Metrics &amp; Alerting","text":"<p>Every good system has metrics and alerting, and my homelab is no different!</p>"},{"location":"metrics/#stack","title":"Stack","text":"<p>My metrics and alerting system makes use of a couple of components:</p> <ul> <li>Grafana for displaying the data</li> <li>Prometheus as a time series database</li> <li>Various exporters<ul> <li><code>prometheus-node-exporter</code> for performance metrics on each node</li> <li><code>fail2ban_exporter</code> for fail2ban jail metrics on zeus</li> <li><code>openvpn_exporter</code> for VPN connection monitoring and logging on zeus</li> <li><code>rtorrent_exporter</code> for monitoring rtorrent data on hermes</li> </ul> </li> </ul>"},{"location":"metrics/#alerting","title":"Alerting","text":"<p>I use Grafana's alerting system to post to a Discord webhook to alert me of any issues.</p> <p>This works quite well, however it displays slightly ugly. There may be a way to pretty it up, but I have yet to find out how.</p>"},{"location":"metrics/#current-alerts","title":"Current Alerts","text":"Alert Name Trigger High CPU Usage 15m load average is greater than the CPU core count minus 1"},{"location":"metrics/fail2ban/","title":"Fail2Ban","text":"<p>Fail2ban is running on my main SSH host Zeus. The service <code>fail2ban_exporter</code> runs on zeus also and exposes metrics collected from the fail2ban socket. Prometheus then collects that information and Grafana displays it.</p>"},{"location":"metrics/fail2ban/#config","title":"Config","text":""},{"location":"metrics/fail2ban/#service-file","title":"Service file","text":"<p>Located in <code>/etc/systemd/system/fail2ban_exporter.service</code></p> <pre><code>[Unit]\nDescription=Fail2Ban Exporter\n\n[Service]\nUser=root\nExecStart=/usr/local/bin/fail2ban_exporter --collector.f2b.socket=/var/run/fail2ban/fail2ban.sock --web.listen-address=\":9191\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"metrics/fail2ban/#networking","title":"Networking","text":"Host Port Endpoint Zeus 9191 \"/metrics\""},{"location":"security/gpgkeys/","title":"GPG Keys","text":"<p>Github uses GPG keys to verify the integrity of the code committed to a repository.</p> <p>There are two keys on my GitHub account, one for Poseidon and one for Perseus.</p> Hostname Date added Poseidon 2020-11-26 Perseus 2022-01-03"},{"location":"services/","title":"Services","text":"<p>I run the vast majority of my services in docker containers orchestrated by nomad. This enhances portability and helps with continuity of service.</p> <p>DistroByte/nomad contains the configuration for any of my jobs that use nomad. There are some environment variables that are populated with consul. I'll link to some oddities and notes I found while running these various services in this section.</p>","tags":["services"]},{"location":"services/ddclient/","title":"DDClient","text":"<p>DDClient is a service that connects with Cloudflare to update my ip address whenever it is needed. By default it does so every 15 minutes.</p> <p>DDClient is managed by the <code>ddclient.hcl</code> job.</p>"},{"location":"services/ddclient/#configuration","title":"Configuration","text":"<p>DDClient doesn't need much to function well, just the zone you want to update, the email address associated with your cloudflare account, and an API key to authorize itself.</p> DDClientNomad <p>The configuration file is mounted into the container from <code>/data/ddclient</code>.</p> <pre><code># Configuration file for ddclient generated by debconf\n#\n# /etc/ddclient.conf\n\ndaemon=1800\nuse=web\nweb=checkip.dyndns.com/\nweb-skip='IP Address'\nssl=yes\n\nprotocol=cloudflare\nssl=yes\nserver=api.cloudflare.com/client/v4\nlogin=[super secret email address]\npassword=[super secret API key]\n\n# james-hackett.ie\nzone=james-hackett.ie\njames-hackett.ie\nca.james-hackett.ie\npaperless.james-hackett.ie\nhome.james-hackett.ie\n\n# dbyte.xyz\nzone=dbyte.xyz\ndbyte.xyz\n\n# distrobyte.xyz\nzone=distrobyte.xyz\ndistrobyte.xyz\n</code></pre> <p>The job file is located at <code>ddclient.hcl</code>.</p> <pre><code>job \"ddclient\" {\n  datacenters = [\"dc1\"]\n\n  type = \"service\"\n\n  group \"ddclient\" {\n    task \"ddclient\" {\n      driver = \"docker\"\n\n      config {\n        image = \"lscr.io/linuxserver/ddclient:latest\"\n\n        volumes = [\n          \"/data/ddclient:/config\"\n        ]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"services/hedgedoc/","title":"HedgeDoc","text":"<p>HedgeDoc is running in a docker container. HedgeDoc is a web editor for markdown notes and contains everything from college notes to personal journal entries and todo lists.</p> <p>It's available here.</p> <p>The repository which the container is from is located on the hedgedoc/container repo.</p>","tags":["services","hedgedoc","backup"]},{"location":"services/hedgedoc/#configuration","title":"Configuration","text":"<p>All configuration is done through nomad. The configuration is located in the nomad/hedgedoc.hcl file.</p> <p>Environment variables control the configuration of the container, with the most important one being the domain.</p>","tags":["services","hedgedoc","backup"]},{"location":"services/hedgedoc/#backup-strategy","title":"Backup Strategy","text":"<p>HedgeDoc is backed up every 3 hours to <code>/backups/hedgedoc/</code> on Dionysus. The files are then kept for 14 days before being removed. If the backup fails, a ping is sent to a Discord channel with the filename and date of the backup.</p> <p>The script which <code>cron</code> runs is shown below. It execs into the hedgedoc-database container and runs <code>pg_dump</code>. This is then sent to the mounted backup folder. The script then removes files older than 14 days and gets the file size of the most recent backup. Finally a notification is sent to Discord if the backup fails, otherwise, the script exits cleanly.</p> <p>The script that runs the backup task is defined in a nomad \"batch\" job here</p> <p>If you just want the bash script that backs up the database, click on the \"Bash script\" tab below.</p> Nomad Job <pre><code>job \"hedgedoc-backup\" {\n  datacenters = [\"dc1\"]\n  type        = \"batch\"\n\n  periodic {\n    cron             = \"0 */3 * * * *\"\n    prohibit_overlap = true\n  }\n\n  group \"db-backup\" {\n    task \"postgres-backup\" {\n      driver = \"raw_exec\"\n\n      config {\n        command = \"/bin/bash\"\n        args    = [\"local/script.sh\"]\n      }\n\n      template {\n        data = &lt;&lt;EOH\n#!/bin/bash\n\nfile=/backups/hedgedoc/hedgedoc-$(date +%Y-%m-%d_%H-%M-%S).sql\n\ndocker exec $(docker ps -aqf \"name=^hedgedoc-db-*\") pg_dump hedgedoc -U hedgedoc &gt; \"${file}\"\n\nfind /backups/hedgedoc/hedgedoc* -ctime +14 -exec rm {} \\;\n\nfile_size=$(find $file -exec du -sh {} \\; | cut -f1 | xargs | sed 's/$//')\n\nif test -f \"$file\"; then\n  exit 0\nelse\n  curl -H \"Content-Type: application/json\" -d '{\"content\": \"`HedgeDoc` backup has just **FAILED**\\nFile name: `'\"$file\"'`\\nDate: `'\"$(TZ=Europe/Dublin date)\"'`\"}' {{ key \"discord/log/webhook\" }}\nfi\nEOH\n\n        destination = \"local/script.sh\"\n      }\n\n      template {\n        data = &lt;&lt;EOH\n# as service 'db-task' is registered in Consul\n# we wat to grab its 'alloc' tag\n{{- range $tag, $services := service \"db-task\" | byTag -}}\n{{if $tag | contains \"alloc\"}}\n{{$allocId := index ($tag | split \"=\") 1}}\nDB_ALLOC_ID=\"{{ $allocId }}\"\n{{end}}\n{{end}}\n        EOH\n        destination = \"secrets/file.env\"\n        env         = true\n      }\n    }\n  }\n}\n</code></pre> Bash script <pre><code>#!/bin/bash\n\nfile=/backups/hedgedoc/hedgedoc-$(date +%Y-%m-%d_%H-%M-%S).sql\n\ndocker exec $(docker ps -aqf \"name=^hedgedoc-db-*\") pg_dump hedgedoc -U hedgedoc &gt; \"${file}\"\n\nfind /backups/hedgedoc/hedgedoc* -ctime +14 -exec rm {} \\;\n\nfile_size=$(find $file -exec du -sh {} \\; | cut -f1 | xargs | sed 's/$//')\n\nif test -f \"$file\"; then\n  exit 0\nelse\n  curl -H \"Content-Type: application/json\" -d '{\"content\": \"`HedgeDoc` backup has just **FAILED**\\nFile name: `'\"$file\"'`\\nDate: `'\"$(TZ=Europe/Dublin date)\"'`\"}' {{ key \"discord/log/webhook\" }}\n  # `{{ key \"discord/log/webhook\" }}` is the webhook URL for the Discord channel\nfi\n</code></pre>","tags":["services","hedgedoc","backup"]},{"location":"services/media/","title":"Media Stack","text":"<p>I run quite a large stack for downloading, indexing and watching media over the internt. All of these services run on Dionysus. The stack includes:</p> <ul> <li>ruTorrent</li> <li>OpenVPN</li> <li>Sonarr/Radarr</li> <li>Overseerr</li> <li>Jackett</li> <li>Plex</li> <li>Tautulli</li> </ul> <p>These services all contribute to allow me to watch TV shows and movies very easily wherever I am in the world.</p> <p>They are all managed by one large <code>docker-compose.yml</code> file, but I will go through each service individually.</p> <p>The flow chart from adding a movie to watching it looks like this:</p> <p></p>"},{"location":"services/media/#general-configuration","title":"General Configuration","text":"<p>These services are all located in <code>/etc/docker-compose/plex</code> on Dionysus, and thus are quite easy to configure all together using environment variables. The config for each service is located in <code>${ROOT}/config/$service_name</code> and any other directory the service needs is created before starting the containers.</p>"},{"location":"services/media/#note","title":"Note","text":"<p>I won't dive into each individual service's configuration, that will be done on their respective pages. This will just contain the basics of how to get the service running.</p>"},{"location":"services/media/#rutorrent","title":"ruTorrent","text":"<p>ruToorent is a torrenting client with some nice features over other torrenting clients of similar standing. I'm using the container from linuxserver/rutorrent. There is a compose file on that page, but I had to adapt mine slightly to make use of the VPN I wanted to use.</p>"},{"location":"services/media/#configuration","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse, but it does not have to be complicated.</p> <pre><code>rutorrent:\n  image: linuxserver/rutorrent\n  container_name: rutorrent\n  network_mode: service:vpn\n  environment:\n    - PUID=${PUID}\n    - PGID=${PGID}\n  volumes:\n    - ${ROOT}/config/rutorrent:/config\n    - ${ROOT}/downloads:/downloads\n  restart: always\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p>"},{"location":"services/media/#points-to-note","title":"Points to Note","text":"<p>The container will not start unless the VPN has also started, thus it is one of the last services to come up when <code>docker-compose up -d</code> is run.</p>"},{"location":"services/media/#backups","title":"Backups","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#openvpn","title":"OpenVPN","text":"<p>OpenVPN is what allows my torrenting to be anonymous (whoops, let that one slip). I have it configured to point to the Netherlands with PrivateInternetAccess. I have had no issues with them, and I have it installed on all my devices too. OpenVPN uses the dperson/openvpn-client container.</p>"},{"location":"services/media/#configuration_1","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse here, but it does not have to be complicated.</p> <pre><code>vpn:\n  container_name: vpn\n  image: dperson/openvpn-client\n  cap_add:\n    - net_admin\n  restart: always\n  volumes:\n    - /dev/net:/dev/net:z\n    - ${ROOT}/config/vpn:/vpn\n  security_opt:\n    - label:disable\n  ports:\n    - 8111:80\n    - 9117:9117\n  command: '-f \"\" -r 192.168.1.0/24'\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p> <p>Because I am running this on a Synology, OpenVPN has some issues creating a tunnel. I had to create a file to open the tunnel whenever the machine restarted. It's a simple bash script that Synology runs on startup.</p> <pre><code>#!/bin/sh\n\n# Create the necessary file structure for /dev/net/tun\nif ( [ ! -c /dev/net/tun ] ); then\n  if ( [ ! -d /dev/net ] ); then\n    mkdir -m 755 /dev/net\n  fi\n  mknod /dev/net/tun c 10 200\nfi\n\n# Load the tun module if not already loaded\nif ( !(lsmod | grep -q \"^tun\\s\") ); then\n  insmod /lib/modules/tun.ko\nfi\n</code></pre> <p>There are some config files that the VPN needs. They are placed in <code>config/vpn</code>. These files are:</p> <ul> <li><code>vpn.auth</code></li> <li><code>vpn.conf</code></li> <li><code>ca.rsa.2048.crt</code></li> <li><code>crl.rsa.2048.pem</code></li> </ul>"},{"location":"services/media/#backup","title":"Backup","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#sonarrradarr","title":"Sonarr/Radarr","text":"<p>Sonarr and Radarr provide a well designed interface to interact with the data from torrent indexers, like Jackett. They interact with both the download client (deluge) and the indexer (jackett) to make it as simple as possible to get the correct quality and release.</p>"},{"location":"services/media/#configuration_2","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse here, but it does not have to be complicated.</p> <pre><code>sonarr:\n  container_name: sonarr\n  image: linuxserver/sonarr\n  restart: always\n  network_mode: host\n  environment:\n    - PUID=${PUID}\n    - PGID=${PGID}\n    - TZ=${TIMEZONE}\n  volumes:\n    - /etc/localtime:/etc/localtime:ro\n    - ${ROOT}/config/sonarr:/config\n    - /volume1/media/tv:/tv\n    - ${ROOT}/downloads:/downloads\n\nradarr:\n  container_name: radarr\n  image: linuxserver/radarr\n  restart: always\n  network_mode: host\n  environment:\n    - PUID=${PUID}\n    - PGID=${PGID}\n    - TZ=${TIMEZONE}\n  volumes:\n    - /etc/localtime:/etc/localtime:ro\n    - ${ROOT}/config/radarr:/config\n    - ${ROOT}/downloads:/downloads\n    - /volume1/media/movies:/movies\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p>"},{"location":"services/media/#backup_1","title":"Backup","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#overseerr","title":"Overseerr","text":"<p>Overseerr is very similar to both Sonarr and Radarr, but it interacts with both at once to just have one streamlined interface for both movies and tv shows. This acts in a similar way to Sonarr/Radarr asking Jackett for torrents, but you can set limits for users on it. It is also compatible with both desktop and mobile.</p>"},{"location":"services/media/#configuration_3","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse here, but it does not have to be complicated.</p> <pre><code>overseerr:\n  container_name: overseerr\n  image: linuxserver/overseerr\n  restart: unless-stopped\n  ports:\n    - 5055:5055\n  environment:\n    - TZ=${TIMEZONE}\n  volumes:\n    - ${ROOT}/config/overseerr:/app/config\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p>"},{"location":"services/media/#backup_2","title":"Backup","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#jackett","title":"Jackett","text":"<p>Jackett is a torrent indexer. It grabs the torrent links from specified websites and provides them (over an api) to Sonarr and Radarr. It is a very powerful piece of software and is the key to the entire media stack.</p>"},{"location":"services/media/#configuration_4","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse here, but it does not have to be complicated.</p> <pre><code>jackett:\n  container_name: jackett\n  image: linuxserver/jackett\n  restart: always\n  network_mode: service:vpn\n  environment:\n    - PUID=${PUID}\n    - PGID=${PGID}\n    - TZ=${TIMEZONE}\n  volumes:\n    - /etc/localtime:/etc/localtime:ro\n    - ${ROOT}/downloads/blackhole:/downloads\n    - ${ROOT}/config/jackett:/config\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p>"},{"location":"services/media/#backup_3","title":"Backup","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#plex","title":"Plex","text":"<p>Plex is the app that allows me to view my media. It is an extremely powerful media center with auto searching of folders, auto grabbing of posters, transcoding built in, ability to sign in with a Google account and so much more. Plex is available to me anywhere in the world on both the web and the mobile app.</p>"},{"location":"services/media/#configuration_5","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse here, but it does not have to be complicated.</p> <pre><code>plex:\n  container_name: plex\n  image: plexinc/pms-docker\n  restart: unless-stopped\n  network_mode: host\n  environment:\n    - TZ=${TIMEZONE}\n    - PLEX_CLAIM=${PLEX_CLAIM}\n  volumes:\n    - ${ROOT}/config/plex/db:/config\n    - ${ROOT}/config/plex/transcode:/transcode\n    - /volume1/media:/data\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p>"},{"location":"services/media/#backup_4","title":"Backup","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#tautulli","title":"Tautulli","text":"<p>Tautulli is a statistic tracker for your local plex installation. It tracks everything from current users to what the most popular TV show and Movies are, who has watched them, and how many hours have they been played for.</p>"},{"location":"services/media/#configuration_6","title":"Configuration","text":"<p>The <code>docker-compose.yml</code> file is quite sparse here, but it does not have to be complicated.</p> <pre><code>tautulli:\n  container_name: tautulli\n  image: tautulli/tautulli\n  restart: unless-stopped\n  network_mode: host\n  volumes:\n    - ${ROOT}/config/tautilli:/config\n  environment:\n    - PUID=${PUID}\n    - PGID=${PGID}\n    - TZ=${TIMEZONE}\n  ports:\n    - 8181:8181\n</code></pre> <p>NOTE This is just a snippet of the whole file, see below for the entire file.</p>"},{"location":"services/media/#backup_5","title":"Backup","text":"<p>This config is backed up by default as it exists on Dionysus, however, I plan to make offsite backups and this configuration would be included in it.</p>"},{"location":"services/media/#entire-file","title":"Entire File","text":"<pre><code>version: '3.7'\n\nservices:\n  vpn:\n    container_name: vpn\n    image: dperson/openvpn-client\n    cap_add:\n      - net_admin\n    restart: always\n    volumes:\n      - /dev/net:/dev/net:z\n      - ${ROOT}/config/vpn:/vpn\n    security_opt:\n      - label:disable\n    ports:\n      - 8111:80\n      - 9117:9117\n    command: '-f \"\" -r 192.168.1.0/24'\n\n  plex:\n    container_name: plex\n    image: plexinc/pms-docker:beta\n    restart: always\n    network_mode: host\n    environment:\n      - TZ=${TIMEZONE}\n      - PLEX_CLAIM=${PLEX_CLAIM}\n    volumes:\n      - ${ROOT}/config/plex/db:/config\n      - ${ROOT}/config/plex/transcode:/transcode\n      - /volume1/media:/data\n    devices:\n      - /dev/dri:/dev/dri\n\n  sonarr:\n    container_name: sonarr\n    image: linuxserver/sonarr\n    restart: always\n    network_mode: host\n    environment:\n      - PUID=${PUID}\n      - PGID=${PGID}\n      - TZ=${TIMEZONE}\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ${ROOT}/config/sonarr:/config\n      - /volume1/media/tv:/tv\n      - ${ROOT}/downloads:/downloads\n\n  radarr:\n    container_name: radarr\n    image: linuxserver/radarr\n    restart: always\n    network_mode: host\n    environment:\n      - PUID=${PUID}\n      - PGID=${PGID}\n      - TZ=${TIMEZONE}\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ${ROOT}/config/radarr:/config\n      - ${ROOT}/downloads:/downloads\n      - /volume1/media/movies:/movies\n\n  jackett:\n    container_name: jackett\n    image: linuxserver/jackett\n    restart: always\n    network_mode: service:vpn\n    environment:\n      - PUID=${PUID}\n      - PGID=${PGID}\n      - TZ=${TIMEZONE}\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ${ROOT}/downloads/blackhole:/downloads\n      - ${ROOT}/config/jackett:/config\n\n  rutorrent:\n    image: linuxserver/rutorrent\n    container_name: rutorrent\n    network_mode: service:vpn\n    environment:\n      - PUID=${PUID}\n      - PGID=${PGID}\n    volumes:\n      - ${ROOT}/config/rutorrent:/config\n      - ${ROOT}/downloads:/downloads\n    restart: always\n</code></pre>"},{"location":"services/paperless/","title":"Paperless-ngx","text":"<p>Paperless-ngx is a place where scanned documents like bills and statements can be stored to reduce the amount of paper in use. It can automatically tag a new document when it arrives and provides OCR on the document to allow it to be searched easily.</p> <p>It is available here.</p>"},{"location":"services/paperless/#configuration","title":"Configuration","text":"<p>Running on nomad, the job file is located at <code>paperless.hcl</code>.</p>"},{"location":"services/paperless/#backup-strategy","title":"Backup Strategy","text":"<p>Paperless is backed up once a week to <code>/volume1/backups/paperless</code> on Dionysus. The way Paperless-ng handles exporting documents allows incremental backups, this is how it is implemented here. It runs once a week.</p> <p>This script execs into the paperless webserver and runs the <code>document_exporter</code> command. This exports all the info to <code>../export</code> which is mounted on the file system. The <code>$file</code> variable contains the size of the export. The data is then copied into the mounted folder and a notification to Discord is sent.</p> <pre><code>#!/bin/bash\n\n# $file - location of the mounted export dir in the paperless container - see\n# here: https://github.com/DistroByte/nomad/blob/master/paperless.hcl#L54\n\ndocker exec $(docker ps -aqf \"name=^paperless-webserver-*\") bash document_exporter ../export\n\nfile=$(du -sh /data/paperless/export/ | cut -f1 | xargs | sed 's/$//')\n\nif test \"$file\"; then\n  exit 0\nelse\n  curl -H \"Content-Type: application/json\" -d '{\"content\": \"`Paperless` backup has just **FAILED**\\nFile size: `'\"$file\"'`\\nDate: `'\"$(TZ=Europe/Dublin date)\"'`\"}' https://canary.discord.com/api/webhooks/$webhook_url\nfi\n</code></pre>"},{"location":"services/service-list/","title":"Overview","text":"<p>This section will list all the services I run inside my homelab, how they are implemented (configs, notes, etc) and links to documentation on the service.</p> <p>In theory, I should be able to recreate my entire service stack from these docs alone.</p>"},{"location":"services/service-list/#list-of-services","title":"List of Services","text":"<ul> <li>DDClient</li> <li>HedgeDoc</li> <li>ruTorrent</li> <li>OpenVPN</li> <li>Sonarr/Radarr</li> <li>Overseerr</li> <li>Jackett</li> <li>Plex</li> <li>Tautilli</li> <li>Paperless-ngx</li> <li>Unifi-Controller</li> <li>Homarr</li> <li>Pihole</li> </ul>"},{"location":"services/unifi-controller/","title":"Unifi-Controller","text":"<p>Unifi-Controller is running in docker on Hermes and acts as the interface to interact with Hades. It is available to access from inside the network on port <code>8443</code> on Hermes, or from this link.</p> <p>This container is built from the linuxserver/unifi-controller image.</p>"},{"location":"services/unifi-controller/#configuration","title":"Configuration","text":"Docker Compose <p>The docker-compose file for this container is located at <code>/etc/docker-compose/unifi</code> on Hermes.</p> <p>The actual config for the controller is located in the same directory as the compose file.</p> <pre><code>version: \"2.1\"\n\nservices:\nunifi-controller:\n    image: ghcr.io/linuxserver/unifi-controller\n    container_name: unifi\n    environment:\n    - PUID=1000\n    - PGID=1000\n    volumes:\n    - /etc/docker-compose/unifi/config:/config\n    ports:\n    - 3478:3478/udp\n    - 10001:10001/udp\n    - 8080:8080/tcp\n    - 8443:8443/tcp\n    restart: unless-stopped\n</code></pre>"},{"location":"services/unifi-controller/#backup-strategy","title":"Backup Strategy","text":"<p>The Unifi Controller container has a function to auto backup its configs at set intervals. These backups are stored in <code>/etc/docker-compose/unifi/config/data/backup/autobackup</code>. The controller backs up its config every week (Monday at 1am UTC) and retains the configs for 28 days.</p> <p>The backup files are then synced with <code>/volume1/backups/unifi</code> on Dionysus with a cron job that runs every day.</p> <p>Cron calls this script when it runs:</p> <pre><code>#!/bin/bash\n\nfile=$(find /etc/docker-compose/unifi/config/data/backup/autobackup/* -type f\n-ctime -1 | grep \".*\\.unf$\")\n\nif test -f \"$file\"; then\n    cp $file /etc/docker-compose/unifi/backup/\n\n    find /etc/docker-compose/unifi/backup/autobackup* -ctime +28 -exec rm {} \\;\n\n    filesize=$(du -sh $file | cut -f1 | xargs | sed 's/$//')\n\n    curl -H \"Content-Type: application/json\" -d '{\"content\": \"-----------------\\n\n    **Unifi Backup**\\n-----------------\\n`Unifi` has just been backed up!\\nFile\n    size: `'\"$filesize\"'`\\nDate: `'\"$(TZ=Europe/Dublin date)\"'`\"}'\n    https://canary.discord.com/api/webhooks/$webhook_url\nfi\n</code></pre>"}]}